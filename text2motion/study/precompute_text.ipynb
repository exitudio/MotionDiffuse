{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import utils.paramUtil as paramUtil\n",
    "from datasets import Text2MotionDataset\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from models.transformer import set_requires_grad\n",
    "from datasets import build_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4888/4888 [00:02<00:00, 2332.67it/s]\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "        self.checkpoints_dir = '../checkpoints'\n",
    "        self.dataset_name = 'kit'\n",
    "        self.name = './precompute_text'\n",
    "        self.times = 1\n",
    "        self.debug = False\n",
    "        self.is_train = True\n",
    "        self.feat_bias = 25\n",
    "opt = Temp()\n",
    "\n",
    "opt.save_root = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)\n",
    "opt.meta_dir = pjoin(opt.save_root, 'meta')\n",
    "\n",
    "os.makedirs(opt.meta_dir, exist_ok=True)\n",
    "\n",
    "if opt.dataset_name == 't2m':\n",
    "    opt.data_root = '../data/HumanML3D'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 22\n",
    "    opt.max_motion_length = 196\n",
    "    dim_pose = 263\n",
    "    kinematic_chain = paramUtil.t2m_kinematic_chain\n",
    "elif opt.dataset_name == 'kit':\n",
    "    opt.data_root = '../data/KIT-ML'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 21\n",
    "    dim_pose = 251\n",
    "    opt.max_motion_length = 196\n",
    "    kinematic_chain = paramUtil.kit_kinematic_chain\n",
    "\n",
    "else:\n",
    "    raise KeyError('Dataset Does Not Exist')\n",
    "\n",
    "mean = np.load(pjoin(opt.data_root, 'Mean.npy'))\n",
    "std = np.load(pjoin(opt.data_root, 'Std.npy'))\n",
    "\n",
    "train_split_file = pjoin(opt.data_root, 'train.txt')\n",
    "train_dataset = Text2MotionDataset(opt, mean, std, train_split_file, opt.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(\n",
    "        train_dataset,\n",
    "        samples_per_gpu=128,\n",
    "        drop_last=True,\n",
    "        workers_per_gpu=4,\n",
    "        shuffle=True,\n",
    "        dist=False,\n",
    "        num_gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        self.clip, _ = clip.load('ViT-B/32', \"cpu\")\n",
    "        set_requires_grad(self.clip, False)\n",
    "    \n",
    "    def forward(self, text, device):\n",
    "        with torch.no_grad():\n",
    "            # [info] token of text.shape = [b, 77]. The tokens after the text input is \"0\". \"<|startoftext|>\" and \"<|endoftext|>\" are added. The unknow words \"asldfjh\" takes more than 1 tokens.\n",
    "            text = clip.tokenize(text, truncate=True).to(device)\n",
    "            x = self.clip.token_embedding(text).type(self.clip.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "            x = x + self.clip.positional_embedding.type(self.clip.dtype)\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            x = self.clip.transformer(x)\n",
    "            x = self.clip.ln_final(x).type(self.clip.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motiondiffuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "068dfdc4ffb965f4573c0e5a911e4fc51fbf857f1e34ca53b3b29ead84b5c775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
