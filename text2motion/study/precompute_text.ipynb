{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import utils.paramUtil as paramUtil\n",
    "from datasets import Text2MotionDataset\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from models.transformer import set_requires_grad\n",
    "from datasets import build_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "        self.checkpoints_dir = '../checkpoints'\n",
    "        self.dataset_name = 't2m'\n",
    "        self.name = './precompute_text'\n",
    "        self.times = 1\n",
    "        self.debug = False\n",
    "        self.is_train = True\n",
    "        self.feat_bias = 25\n",
    "opt = Temp()\n",
    "\n",
    "opt.save_root = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)\n",
    "opt.meta_dir = pjoin(opt.save_root, 'meta')\n",
    "\n",
    "os.makedirs(opt.meta_dir, exist_ok=True)\n",
    "\n",
    "if opt.dataset_name == 't2m':\n",
    "    opt.data_root = '../data/HumanML3D'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 22\n",
    "    opt.max_motion_length = 196\n",
    "    dim_pose = 263\n",
    "    kinematic_chain = paramUtil.t2m_kinematic_chain\n",
    "elif opt.dataset_name == 'kit':\n",
    "    opt.data_root = '../data/KIT-ML'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 21\n",
    "    dim_pose = 251\n",
    "    opt.max_motion_length = 196\n",
    "    kinematic_chain = paramUtil.kit_kinematic_chain\n",
    "\n",
    "else:\n",
    "    raise KeyError('Dataset Does Not Exist')\n",
    "\n",
    "mean = np.load(pjoin(opt.data_root, 'Mean.npy'))\n",
    "std = np.load(pjoin(opt.data_root, 'Std.npy'))\n",
    "\n",
    "train_split_file = pjoin(opt.data_root, 'train.txt')\n",
    "# train_dataset = Text2MotionDataset(opt, mean, std, train_split_file, opt.times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I stopped here. Trying to move out from Text2MotionDataset \n",
    "- HumanML3D adds caption in specific duration (start-stop frames) and create new files name on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs as cs\n",
    "from tqdm import tqdm\n",
    "split_file = train_split_file\n",
    "\n",
    "id_list = []\n",
    "with cs.open(split_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        id_list.append(line.strip())\n",
    "if opt.debug:\n",
    "    id_list = id_list[:300]\n",
    "\n",
    "new_name_list = []\n",
    "length_list = []\n",
    "# for name in tqdm(id_list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        self.clip, _ = clip.load('ViT-B/32', \"cpu\")\n",
    "        set_requires_grad(self.clip, False)\n",
    "    \n",
    "    def forward(self, text, device):\n",
    "        with torch.no_grad():\n",
    "            # [info] token of text.shape = [b, 77]. The tokens after the text input is \"0\". \"<|startoftext|>\" and \"<|endoftext|>\" are added. The unknow words \"asldfjh\" takes more than 1 tokens.\n",
    "            text = clip.tokenize(text, truncate=True).to(device)\n",
    "            x = self.clip.token_embedding(text).type(self.clip.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "            x = x + self.clip.positional_embedding.type(self.clip.dtype)\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            x = self.clip.transformer(x)\n",
    "            x = self.clip.ln_final(x).type(self.clip.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption, motion, m_length in train_dataset:\n",
    "    print(caption)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motiondiffuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "068dfdc4ffb965f4573c0e5a911e4fc51fbf857f1e34ca53b3b29ead84b5c775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
