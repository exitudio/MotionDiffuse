{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import utils.paramUtil as paramUtil\n",
    "from options.train_options import TrainCompOptions\n",
    "from utils.plot_script import *\n",
    "\n",
    "from datasets import Text2MotionDataset\n",
    "\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.parallel import MMDistributedDataParallel, MMDataParallel\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from mylib import animate3d, kit_bone, kit_kit_bone, visualize_2motions\n",
    "from utils.motion_process import recover_from_ric\n",
    "from datasets import build_dataloader\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mask_model.motiontransformer import MotionTransformerOnly, generate_src_mask\n",
    "from mask_model.quantize import VectorQuantizer2\n",
    "from mask_model.util import hinge_d_loss, vanilla_d_loss, MeanMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 2645.90it/s]\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self):\n",
    "        print('mock:: opt')\n",
    "opt = Temp()\n",
    "\n",
    "\n",
    "# mock from /home/epinyoan/git/MotionDiffuse/text2motion/options/train_options.py\n",
    "opt.num_layers = 8\n",
    "opt.latent_dim = 512\n",
    "opt.diffusion_steps = 1000\n",
    "opt.no_clip = False\n",
    "opt.no_eff = False\n",
    "opt.num_epochs = 50\n",
    "opt.lr = 2e-4\n",
    "# opt.batch_size = 32\n",
    "opt.times = 1\n",
    "opt.feat_bias = 1\n",
    "opt.is_continue = False\n",
    "opt.log_every = 50\n",
    "opt.save_every_e = 5\n",
    "opt.log_every = 5\n",
    "opt.eval_every_e = 5\n",
    "opt.save_latest = 500\n",
    "opt.is_train = True\n",
    "\n",
    "# mock from cmd\n",
    "opt.name = 'TEMP'\n",
    "opt.batch_size = 2 # [INFO] modified\n",
    "opt.times = 50\n",
    "opt.num_epochs = 1 # [INFO] modified\n",
    "opt.dataset_name = 'kit' # [INFO] or t2m or kit\n",
    "opt.num_layers = 8\n",
    "opt.diffusion_steps = 1000\n",
    "opt.data_parallel = True\n",
    "opt.gpu_id = [0,1,2,3]\n",
    "opt.device = torch.device(\"cuda\")\n",
    "\n",
    "# mock from /home/epinyoan/git/MotionDiffuse/text2motion/options/base_options.py\n",
    "opt.checkpoints_dir = '../checkpoints'\n",
    "opt.distributed = False\n",
    "\n",
    "opt.save_root = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)\n",
    "opt.model_dir = pjoin(opt.save_root, 'model')\n",
    "opt.meta_dir = pjoin(opt.save_root, 'meta')\n",
    "\n",
    "if opt.dataset_name == 't2m':\n",
    "    opt.data_root = '../data/HumanML3D' # change root to '../'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 22\n",
    "    radius = 4\n",
    "    fps = 20\n",
    "    opt.max_motion_length = 196\n",
    "    dim_pose = 263\n",
    "    kinematic_chain = paramUtil.t2m_kinematic_chain\n",
    "elif opt.dataset_name == 'kit':\n",
    "    opt.data_root = '../data/KIT-ML' # change root to '../'\n",
    "    opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "    opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "    opt.joints_num = 21\n",
    "    radius = 240 * 8\n",
    "    fps = 12.5\n",
    "    dim_pose = 251\n",
    "    opt.max_motion_length = 196\n",
    "    kinematic_chain = paramUtil.kit_kinematic_chain\n",
    "\n",
    "opt.debug = True # [INFO] added\n",
    "opt.corrupt = 'diffusion' # [INFO] added ['mask', 'diffusion']\n",
    "dim_word = 300\n",
    "mean = np.load(pjoin(opt.data_root, 'Mean.npy'))\n",
    "std = np.load(pjoin(opt.data_root, 'Std.npy'))\n",
    "\n",
    "train_split_file = pjoin(opt.data_root, 'train.txt')\n",
    "\n",
    "\n",
    "train_dataset = Text2MotionDataset(opt, mean, std, train_split_file, opt.times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Quantization (CodeBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 196, 251])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq = VectorQuantizer2(n_e = 1024,\n",
    "                      e_dim = dim_pose)\n",
    "# quant, emb_loss, info = vq(torch.rand([5, 256, 16, 16]))\n",
    "z_q = vq(torch.rand([2, 196, dim_pose]))\n",
    "z_q.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "Something to look further\n",
    "- length set to \"MotionTransformer\" ex. discriminator(recon, length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(\n",
    "        train_dataset,\n",
    "        samples_per_gpu=opt.batch_size,\n",
    "        drop_last=True,\n",
    "        workers_per_gpu=1,\n",
    "        shuffle=True,\n",
    "        dist=opt.distributed,\n",
    "        num_gpus=1)\n",
    "text, motions, length = next(iter(train_loader))\n",
    "motions = motions.float()\n",
    "src_mask = generate_src_mask(motions.shape[1], length).to(motions.device).unsqueeze(-1)\n",
    "mean_mask = MeanMask(src_mask, dim_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_q torch.Size([2, 196, 256])\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "encoder = MotionTransformerOnly(input_feats=dim_pose, latent_dim=latent_dim, num_layers=8)\n",
    "decoder = MotionTransformerOnly(input_feats=latent_dim, output_feats=dim_pose, latent_dim=latent_dim, num_layers=8)\n",
    "discriminator = MotionTransformerOnly(input_feats=dim_pose, output_feats=1, latent_dim=latent_dim, num_layers=4)\n",
    "q_beta = 0.25\n",
    "quantize = VectorQuantizer2(n_e = 1024,\n",
    "                            e_dim = latent_dim)\n",
    "\n",
    "z = encoder(motions, src_mask=src_mask, length=length)\n",
    "z_q = quantize(z) * src_mask\n",
    "qloss = mean_mask.mean((z_q.detach()-z)**2 * src_mask) + 0.25 * \\\n",
    "                   mean_mask.mean((z_q - z.detach()) ** 2 * src_mask)\n",
    "z_q = z + (z_q - z).detach()\n",
    "recon = decoder(z_q, src_mask=src_mask, length=length)\n",
    "logits_fake = discriminator(recon, src_mask=src_mask, length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8255, grad_fn=<DivBackward0>),\n",
       " tensor(0.8255, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = torch.abs(motions - recon)\n",
    "a = (diff * src_mask).sum()/(src_mask.sum() * dim_pose)\n",
    "# src_mask.sum(), src_mask.shape\n",
    "mean_mask = MeanMask(src_mask, dim_pose)\n",
    "mean_mask.mean(diff * src_mask), a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GAN loss\n",
    "# [TODO] mask the shorter frames for gan loss\n",
    "rec_loss = mean_mask.mean(torch.abs(motions - recon) * src_mask)\n",
    "# [TODO] clarify: discriminator of VQGAN output only 1x30x30 from input 3x256x256\n",
    "g_loss = -mean_mask.mean(logits_fake)\n",
    "# [TODO] skip perceptual loss (LPIPS)\n",
    "d_weight = 1 # [TODO] skip calculate_adaptive_weight\n",
    "disc_factor = 1 # [TODO] adopt_weight\n",
    "loss = rec_loss + d_weight * disc_factor * g_loss + qloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Discriminator loss\n",
    "# [TODO] mask the shorter frames for dis loss\n",
    "logits_real = discriminator(motions.detach(), src_mask=src_mask, length=length) # [TODO] Can we use the same logits_real from GAN loss??? \n",
    "logits_fake = discriminator(recon.detach(), src_mask=src_mask, length=length)\n",
    "d_loss = disc_factor * vanilla_d_loss(logits_real, logits_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4.5e-6\n",
    "opt_ae = torch.optim.Adam(list(encoder.parameters())+\n",
    "                            list(decoder.parameters())+\n",
    "                            list(quantize.parameters()),\n",
    "                            lr=lr, betas=(0.5, 0.9))\n",
    "opt_disc = torch.optim.Adam(discriminator.parameters(),\n",
    "                            lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss.backward()\n",
    "d_loss.backward()\n",
    "opt_ae.step()\n",
    "opt_disc.step()\n",
    "opt_ae.zero_grad()\n",
    "opt_disc.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motiondiffuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "068dfdc4ffb965f4573c0e5a911e4fc51fbf857f1e34ca53b3b29ead84b5c775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
